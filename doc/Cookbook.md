# Using `2pt_validation` or "How to simulate (and analyze) a catalog as big as LSST's 10 year data in one afternoon (a.k.a. filling up your disk space in a few hours)"

Authors: D. Alonso, J. Sanchez, A. Slosar

This document intends to guide the user in the process of generating mock catalogs and analyzing their 2-point statistics using the scripts provided in this repository and the resources at NERSC (mostly focusing on Cori).

Requirements:

* [CoLoRe](https://github.com/damonge/colore)

* [SACC](https://github.com/LSSTDESC/sacc)

* [fastcat](https://github.com/slosar/fastcat)

* [NaMaster](https://github.com/damonge/namaster)

* [CCL](https://github.com/LSSTDESC/CCL)

* [LSSLike](https://github.com/LSSTDESC/LSSLike)

Each one of the required pieces of software has its own installation procedure. If you find any bugs or problems please contact the authors.

**Warning:** as of Jan 31st 2018 some of these dependencies have been significantly updated. Please make sure to:
- Pull and compile the latest version of CoLoRe (master branch).
- Pull and install the latest version of NaMaster (master branch).
- Pull and install the latest version of fastcat (master branch).
- Pull and install the latest version of CCL (**external_pk** branch. This is the one needed to compute lognormal predictions).

## Step 1) Generation of the cosmological mocks using CoLoRe

[CoLoRe](https://github.com/damonge/colore) is a highly parallel software capable of generating fast cosmological mocks using three different approaches: lognormal, 1-order LPT, and 2-order LPT. These mocks can include different properly correlated tracers with their corresponding bias. CoLoRe is also able to generate the corresponding convergence and shear maps, intensity mapping and ISW effect. All you have to do is to input the proper `param.cfg` file and run `./CoLoRe param.cfg`.

In our case, we restrict our analysis to a single galaxy population. Our current setup includes only a sample of red galaxies similar to what LSST should be able to observe, defined by their N(z) and b(z). We generate 100 realizations of a simulation with 4096^3 grid cells from redshift 0.001 to 1.6 using a Planck-like input cosmology. We select the lognormal method with a 2 Mpc/h smoothing in order to be able to recover accurate theoretical predictions.

To run this part, assuming that you have CoLoRe properly installed and running at Cori, you have to connect to Cori and in a terminal follow these steps:

* Go to the folder `drive_CoLoRe`.
* Open `run_all.sh` with your favorite text editor and modify all relevant lines (the comments in this script should make it self-explanatory).
* Depending on the purpose of your run (e.g. debug) or its size (e.g. n_grid, #gals etc.) you'll have to tweak the queue-related variables.
* Type `/bin/bash run_all.sh` to launch the sims.

Expect each realization to take up ~10 GB of disk space.

**Warning:** we plan on having this process automatized in vc.py, however this is still not ready. As of Jan 31st 2018 no modifications have been made wrt the original version.


## Step 2) Generation of the catalogs

Once we have the cosmological mocks generated by CoLoRe, we want to add several observational effects. In order to do so, we use [fastcat](https://github.com/slosar/fastcat). This program allows the user to include several flavors of depth variations/footprints, photometric redshifts and stellar contamination among others. It is heavily parallelized and able to write the outputs into a single file (using h5py-parallel) or different chunks (recommended). In our case, we restrict our analysis to Gaussian photo-z with a width of 0.05(1+z), and use the depth variations implemented by Humna Awan. We do not include stars in this iteration although we are planning to do so in the near future.

To generate the catalogs you have to do the following:

* Go to the directory `fastcat_CoLore`.
* Open `run_all.sh` in any text editor, and edit it at will. The comments should make it self-explanatory.
* This script makes use of `mkcat.py`. The available command-line options can be found running `python mkcat.py -h`.
* **Warning:** many of the options implemented in `mkcat.py` have not been tested yet. This includes mpi parallelization, photo-z options other than Gaussian, window functions other than "none" and star contamination. Also, currently we can only guarantee unbiased N(z)'s if you include the option `--ztrue`.
* Type `/bin/bash run_all.sh` to process all sims.


## Step 3) Computing the power-spectra using NaMaster

Now that you have your source catalogs you can try to analyze their 2-point statistics. We do so with [NaMaster](https://github.com/damonge/namaster). `NaMaster` is an implementation of the `Master` algorithm by D. Alonso using unbiased pseudo-Cl estimation and able to perform mode projection (among other things). We use `NaMaster` through `namaster_interface.py`. This program reads each one of our `fastcat` catalogs and generates `HEALpix` maps using the binning in redshift specified by an ASCII file that also includes the maximum `l` at which we want to calculate the power-spectra. The program outputs a [SACC](https://github.com/LSSTDESC/SACC) file containing the results for the different tracers and information about the binning, N(z), etc.

To compute the power-spectra you have to follow these steps:

* Go to the directory `namaster_fastcat`.
* Open `run_all.sh` in any text editor, and edit it at will. The comments should make it self-explanatory.
* This script makes use of `namaster_interface.py`. The available command-line options can be found running `python namaster_interface.py -h`.
* **Warning:** some of these options haven't been tested yet. This includes mpi parallelization, estimating the analytic covariance matrix, taking in an external NmtWorkspace object.
* **Warning:** this script was also used in the past to compute the theoretical predictions. This is not the case anymore (see 4 below).
* Type `/bin/bash run_all.sh` to process all sims.


## Step 4) Compute the theoretical prediction

This repo currently also hosts scripts to compute the theoretical predictions for the angular power spectra of the simulations. These are housed in the `theory_comparison` folder. To generate them:
* Go to the directory `theory_comparison`.
* Open `run_all.sh` in any text editor, and edit it at will. The comments should make it self-explanatory.
* This script makes use of `mk_theory.py`. The available command-line options can be found running `python mk_theory.py -h`.
* **Warning:** some of these options haven't been tested yet. This includes mostly the inclusion of RSDs.
* **Warning:** in order to estimate the lognormal prediction, a small C program (`lnpred`) needs to be compiled first. In order to do that, type `make`. You may have to modify the paths to the relevant libraries (GSL, FFTW, CCL) in `Makefile`.
* Type `/bin/bash run_all.sh` to process all sims.

This will generate theoretical predictions for all sims and store them as SACC files (note that all sims should in principle have the same theoretical prediction, but in practice there will be small variations between sims due to the statistical fluctuations in the estimated N(z)s). Have a look at the script `compare.py` to understand how to use the SACC files (both data and theory). This script reads the estimated data and theory power spectra for all sims and compares them (both the Gaussian and lognormal predictions).

**Warning:** the contents of this folder are only temporary. Ideally we'd like to incorporate the calculation of the lognormal prediction within the official LSS likelihood [LSSLike](https://github.com/LSSTDESC/LSSLike). This is work in progress and is currently the most urgent target of this project.


### Final notes and disclaimer

We recommend to use the anaconda distribution in Cori and install the relevant python packages using the option `python setup.py install --user` since this makes the installation/update process easier. Please, post issues or send your questions or comments to any of the authors.
